{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar os modelos pré definidos devem ser pegos no torchvision\n",
    "from torchvision import models\n",
    "# vemos aqui todas as classes que se referem as implementações dos modelos. (em letra maiusculas sào as classes e as letras minusculas  )\n",
    "alexnet = models.AlexNet()\n",
    "dir(models)\n",
    "# cirar uma instância do modelo alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet101(pretrained = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layers do modelo PRÉ TREINADO.**\n",
    "\n",
    "O retorno dessa célula são os layers do modelo pré-treinado. Temos então no contexto da ResNet101, 101 camadas de layers, na prática são 101 filtros e funnções não lineares das quais nossos dados vão ser analisados, por fim temos o layer fc que se segue a produzir os scores para cada um dos dados de saídas, permitindo avaliar o resultado de nossas redes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funções para pré processament de entradas.** \n",
    "\n",
    "Conseguimos então chamar nossa rede pré treinada para avaliar nossas imagens de entrada, criando assim um score para cada classe de imagem.\n",
    "Porém antes de qualquer coisa precisamos pré processar nossas imagens de entrada para que elas tenham o tamanho e valores ideais de modo que estejam NO MESMO FAIXA NÚMERICA\n",
    "\n",
    "Usamos então o modulo torchvision que prove os transformers no qual vai nos permitir rapidamente definir pipelinnes das funções de pré processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.Resize(256), # escalamos a imagem para um formato 256 x 256\n",
    "  transforms.CenterCrop(224), # cortamos a imagem em 224 x 224 ao redor do centro\n",
    "  transforms.ToTensor(), # transformamos nossa entrada em um tensor\n",
    "  transforms.Normalize( # normalizamos o RGB \n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "  )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos aqui o pillow, que permite que tratemos imagens. \n",
    "from PIL import Image\n",
    "img = Image.open(\"../../assets/bobby.jpg\") #atribuimos a imagem a uma variável\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessar nossa imagem de entrada\n",
    "import torch\n",
    "img_t = preprocess(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agora rodar nossa rede pré treinada, para isso é necessário \"setar\" nossa rede para o modo eval que permite que os modelos pré treinados produzam respostas significativas.\n",
    "resnet.eval()\n",
    "out = resnet(batch_t)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que rodamos nossa rede e recebemos nossa resposta, podemos então visualizar o resultado gerado, verificando se a rede neural entendeu e classificou a imagem como um ser humano classificaria, o que é o esperado que tenha acontecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../chapter1/imagenet_classes.txt\") as f:\n",
    "  labels = [line.strip() for line in f.readlines() ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora precisamos encontrar um index que corresponde o máximo score gerado da saída do tensor, podemos fazer isso isso usando a função \"max\" do Pytorch, \n",
    "# encontrando os indices em que o valor máximo ocorreu.\n",
    "\n",
    "_, index = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('golden retriever', 96.57185363769531)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para a partir do index obtido estando no formato tensor, precisamos obter o valor númerico para usar como index.\n",
    "# para isso normalizamos nossas saídas no range de [0,1] e dividir pela soma.\n",
    "\n",
    "percentage = torch.nn.functional.softmax(out,dim=1) [0] * 100\n",
    "labels[index[0]], percentage[index[0]].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
