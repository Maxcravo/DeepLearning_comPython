{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2840018023.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar os modelos pré definidos devem ser pegos no torchvision\n",
    "from torchvision import models\n",
    "# vemos aqui todas as classes que se referem as implementações dos modelos. (em letra maiusculas sào as classes e as letras minusculas  )\n",
    "alexnet = models.AlexNet()\n",
    "dir(models)\n",
    "# cirar uma instância do modelo alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/Faculdade/TCC/Codigos_de_Estudo/Pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/max/Faculdade/TCC/Codigos_de_Estudo/Pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/max/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet101(pretrained = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layers do modelo PRÉ TREINADO.**\n",
    "\n",
    "O retorno dessa célula são os layers do modelo pré-treinado. Temos então no contexto da ResNet101, 101 camadas de layers, na prática são 101 filtros e funnções não lineares das quais nossos dados vão ser analisados, por fim temos o layer fc que se segue a produzir os scores para cada um dos dados de saídas, permitindo avaliar o resultado de nossas redes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funções para pré processament de entradas.** \n",
    "\n",
    "Conseguimos então chamar nossa rede pré treinada para avaliar nossas imagens de entrada, criando assim um score para cada classe de imagem.\n",
    "Porém antes de qualquer coisa precisamos pré processar nossas imagens de entrada para que elas tenham o tamanho e valores ideais de modo que estejam NO MESMO FAIXA NÚMERICA\n",
    "\n",
    "Usamos então o modulo torchvision que prove os transformers no qual vai nos permitir rapidamente definir pipelinnes das funções de pré processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.Resize(256), # escalamos a imagem para um formato 256 x 256\n",
    "  transforms.CenterCrop(224), # cortamos a imagem em 224 x 224 ao redor do centro\n",
    "  transforms.ToTensor(), # transformamos nossa entrada em um tensor\n",
    "  transforms.Normalize( # normalizamos o RGB \n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "  )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos aqui o pillow, que permite que tratemos imagens. \n",
    "from PIL import Image\n",
    "img = Image.open(\"../../assets/bobby.jpg\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
